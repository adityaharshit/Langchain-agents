{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7ab335",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\Day3_Project\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'app'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict, List, Any, Optional\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mapp\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmcp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mcp_tool\n\u001b[32m     17\u001b[39m logger = logging.getLogger(\u001b[34m__name__\u001b[39m)\n\u001b[32m     18\u001b[39m load_dotenv()\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'app'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_postgres import PGVector\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import asyncio\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional\n",
    "import re\n",
    "\n",
    "from app.mcp import mcp_tool\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "load_dotenv()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2edb90b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def query_decomposer_tool(tool_input: dict, context: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Uses OpenAI GPT-4o to decompose complex queries into subqueries and returns prioritized list.\n",
    "    \n",
    "    Args:\n",
    "        tool_input: {\"query\": str, \"max_subqueries\": int}\n",
    "        context: Additional context information\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import openai\n",
    "        from app.config import config\n",
    "        \n",
    "        query = tool_input.get(\"query\", \"\")\n",
    "        max_subqueries = tool_input.get(\"max_subqueries\", 5)\n",
    "        \n",
    "        if not query:\n",
    "            return {\"status\": \"error\", \"error\": \"Query is required\", \"meta\": {}}\n",
    "        \n",
    "        # Initialize OpenAI client\n",
    "        client = openai.AsyncOpenAI(api_key=config.OPENAI_API_KEY)\n",
    "        \n",
    "        # Create prompt for intelligent query decomposition\n",
    "        prompt = f\"\"\"\n",
    "You are an expert research analyst tasked with breaking down complex research questions into manageable sub-questions for systematic investigation.\n",
    "\n",
    "Main Research Query: {query}\n",
    "\n",
    "Your task is to decompose this query into {max_subqueries} focused sub-questions that will enable comprehensive research. Each sub-question should:\n",
    "\n",
    "1. Address a specific aspect of the main query\n",
    "2. Be researchable through academic sources, reports, and reliable data\n",
    "3. Build towards answering the main question\n",
    "4. Be prioritized by importance and logical sequence\n",
    "\n",
    "For each sub-question, provide:\n",
    "- The specific sub-question\n",
    "- Priority level (1 = highest priority, {max_subqueries} = lowest)\n",
    "- Rationale for why this sub-question is important\n",
    "- Expected information type (data, analysis, case studies, etc.)\n",
    "\n",
    "Format your response as a JSON array:\n",
    "[\n",
    "  {{\n",
    "    \"subquery\": \"Specific focused research question\",\n",
    "    \"priority\": 1,\n",
    "    \"rationale\": \"Why this question is important for the overall research\",\n",
    "    \"expected_info_type\": \"data|analysis|case_studies|policy_review|literature_review\",\n",
    "    \"research_scope\": \"global|regional|national|sectoral\"\n",
    "  }}\n",
    "]\n",
    "\n",
    "Generate exactly {max_subqueries} sub-questions, ordered by priority.\n",
    "\"\"\"\n",
    "\n",
    "        try:\n",
    "            # Call OpenAI API\n",
    "            response = await client.chat.completions.create(\n",
    "                model=config.DECOMPOSITION_MODEL,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are an expert research analyst specializing in breaking down complex research questions into systematic investigation plans.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.3,\n",
    "                max_tokens=1500\n",
    "            )\n",
    "            \n",
    "            # Parse the response\n",
    "            response_content = response.choices[0].message.content\n",
    "            \n",
    "            # Extract JSON from the response\n",
    "            import json\n",
    "            json_match = re.search(r'\\[.*\\]', response_content, re.DOTALL)\n",
    "            if json_match:\n",
    "                subqueries_data = json.loads(json_match.group())\n",
    "            else:\n",
    "                subqueries_data = json.loads(response_content)\n",
    "            \n",
    "            # Validate and process the subqueries\n",
    "            subqueries = []\n",
    "            for sq_data in subqueries_data:\n",
    "                if isinstance(sq_data, dict) and \"subquery\" in sq_data:\n",
    "                    subqueries.append({\n",
    "                        \"subquery\": sq_data[\"subquery\"],\n",
    "                        \"priority\": sq_data.get(\"priority\", len(subqueries) + 1),\n",
    "                        \"rationale\": sq_data.get(\"rationale\", \"\"),\n",
    "                        \"expected_info_type\": sq_data.get(\"expected_info_type\", \"analysis\"),\n",
    "                        \"research_scope\": sq_data.get(\"research_scope\", \"global\")\n",
    "                    })\n",
    "            \n",
    "            # Sort by priority\n",
    "            subqueries.sort(key=lambda x: x[\"priority\"])\n",
    "            \n",
    "            return {\n",
    "                \"status\": \"ok\",\n",
    "                \"result\": {\n",
    "                    \"original_query\": query,\n",
    "                    \"subqueries\": subqueries,\n",
    "                    \"total_subqueries\": len(subqueries)\n",
    "                },\n",
    "                \"meta\": {\n",
    "                    \"decomposition_method\": \"openai_gpt4o\",\n",
    "                    \"model_used\": config.DECOMPOSITION_MODEL,\n",
    "                    \"prompt_tokens\": response.usage.prompt_tokens if hasattr(response, 'usage') else 0,\n",
    "                    \"completion_tokens\": response.usage.completion_tokens if hasattr(response, 'usage') else 0\n",
    "                }\n",
    "            }\n",
    "            \n",
    "        except json.JSONDecodeError as e:\n",
    "            logger.error(f\"Failed to parse LLM response as JSON: {e}\")\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"error\": f\"Failed to parse decomposition response: {str(e)}\",\n",
    "                \"meta\": {}\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"OpenAI API call failed: {e}\")\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"error\": f\"Query decomposition failed: {str(e)}\",\n",
    "                \"meta\": {}\n",
    "            }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Query decomposition failed: {e}\")\n",
    "        return {\n",
    "            \"status\": \"error\",\n",
    "            \"error\": f\"Query decomposition failed: {str(e)}\",\n",
    "            \"meta\": {}\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7187ff08",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'asyncio' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m query_decomposer_tool(tool_input, context)\n\u001b[32m     10\u001b[39m     \u001b[38;5;28mprint\u001b[39m(json.dumps(result, indent=\u001b[32m2\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43masyncio\u001b[49m.run(main())\n",
      "\u001b[31mNameError\u001b[39m: name 'asyncio' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    async def main():\n",
    "        tool_input = {\n",
    "            \"query\": \"How can artificial intelligence improve sustainable agriculture in developing countries?\",\n",
    "            \"max_subqueries\": 5\n",
    "        }\n",
    "        context = {}\n",
    "\n",
    "        result = await query_decomposer_tool(tool_input, context)\n",
    "        print(json.dumps(result, indent=2))\n",
    "\n",
    "    asyncio.run(main())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
